{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import newspaper #For finding today's trending topics\n",
    "import datetime as dt \n",
    "from datetime import date\n",
    "#from __future__ import print_function, unicode_literals\n",
    "import spacy #For nlp prep\n",
    "from collections import defaultdict, Counter\n",
    "import shutil\n",
    "import requests #For our REST API calls\n",
    "import schedule\n",
    "import time\n",
    "import tweepy #To Post to Twitter\n",
    "import numpy as np\n",
    "nlp = spacy.load('en') #For NLP later\n",
    "\n",
    "#References: \n",
    "#https://www.pluralsight.com/guides/interesting-apis/build-a-simpsons-quote-bot-with-twilio-mms-frinkiac-and-python\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup your Twitter Account Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "consumer_secret = 'YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY'\n",
    "access_token = 'WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW'\n",
    "access_token_secret = 'ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ'\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Today's Trending Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter topics for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Poll Google trends to get hot noun trends with the newspaper package\n",
    "\n",
    "def get_hot_nouns():\n",
    "    pos_counts = defaultdict(Counter)\n",
    "    \n",
    "    nouns=[]\n",
    "    hots = newspaper.hot() #Get today's trending topics\n",
    "    hots = [' {0}'.format(elem) for elem in hots] #add a space to each one\n",
    "    hots = ''.join(hots) #Join them into one big string\n",
    "    \n",
    "    doc = nlp(hots)\n",
    "    \n",
    "    doc = nlp(hots) #Create Spacy Doc\n",
    "    \n",
    "    for token in doc:\n",
    "        pos_counts[token.pos][token.orth] += 1\n",
    "\n",
    "    for pos_id, counts in sorted(pos_counts.items()):\n",
    "        pos = doc.vocab.strings[pos_id]\n",
    "        for orth_id, count in counts.most_common():\n",
    "            #print(pos, count, doc.vocab.strings[orth_id])\n",
    "            if pos == 'NOUN':\n",
    "                nouns.append(doc.vocab.strings[orth_id])\n",
    "                \n",
    "    nouns = ['{0} '.format(elem) for elem in nouns] #add a space to each one\n",
    "    nouns = ''.join(nouns) #Join them into one big string\n",
    "    return(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hashtags for Twitter Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Poll Google trends to get hot noun trends\n",
    "\n",
    "def get_hot_nouns_hashtagged():\n",
    "    pos_counts = defaultdict(Counter)\n",
    "    \n",
    "    nouns=[]\n",
    "    hots = newspaper.hot() #Get today's trending topics\n",
    "    hots = [' {0}'.format(elem) for elem in hots] #add a space to each one\n",
    "    hots = ''.join(hots) #Join them into one big string\n",
    "    \n",
    "    doc = nlp(hots)\n",
    "    \n",
    "    doc = nlp(hots) #Create Spacy Doc\n",
    "    \n",
    "    for token in doc:\n",
    "        pos_counts[token.pos][token.orth] += 1\n",
    "\n",
    "    for pos_id, counts in sorted(pos_counts.items()):\n",
    "        pos = doc.vocab.strings[pos_id]\n",
    "        for orth_id, count in counts.most_common():\n",
    "            #print(pos, count, doc.vocab.strings[orth_id])\n",
    "            if pos == 'NOUN':\n",
    "                nouns.append(doc.vocab.strings[orth_id])\n",
    "                \n",
    "    nouns = ['#{0} '.format(elem) for elem in nouns] #add a space to each one\n",
    "    nouns = ''.join(nouns) #Join them into one big string\n",
    "    return(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Destiny #Release #Date #office '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hot_nouns_hashtagged()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Your Twitter Caption w/Frinkiac API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_meme_caption():\n",
    "    hot_nouns = get_hot_nouns()\n",
    "    url_image = \"https://frinkiac.com/api/search?q=\" + hot_nouns\n",
    "    r = requests.get(url_image) \n",
    "    json = r.json() \n",
    "        # Extract the episode number and timestamp from the API response \n",
    "        # and convert them both to strings. \n",
    "    ID, episode, timestamp = map(str, json[0].values()) \n",
    "    image_url = \"https://frinkiac.com/meme/\" + episode + \"/\" + timestamp \n",
    "         #Combine each line of subtitles into one string. \n",
    "    #caption = \"\\n\".join([subtitle[\"Content\"] for subtitle in json[\"Subtitles\"]]) \n",
    "\n",
    "    url_caption = \"http://frinkiac.com/api/caption?e=\" + episode +\"&t=\" + timestamp\n",
    "    r = requests.get(url_caption) \n",
    "    json = r.json() \n",
    "    ID, episode, timestamp = map(str, json[\"Frame\"].values())  \n",
    "    image_url = \"https://frinkiac.com/meme/\" + episode + \"/\" + timestamp \n",
    "        # Combine each line of subtitles into one string. \n",
    "    caption = \" \".join([subtitle[\"Content\"] for subtitle in json[\"Subtitles\"]])\n",
    "    j=0\n",
    "    hot_nouns_hashtag = []\n",
    "    hot_noun_hashtag = []\n",
    "    for hot_noun in hot_nouns:\n",
    "        hot_noun_hashtag = (\"#\" + hot_nouns[j])\n",
    "        hot_nouns_hashtag.append(hot_noun_hashtag)\n",
    "        j=j+1\n",
    "    \n",
    "    hot_nouns_hashtagged = get_hot_nouns_hashtagged()\n",
    "    caption = \"Simpsons Bot Says: \" + caption + hot_nouns_hashtagged\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    with open('img.png', 'wb') as out_file:\n",
    "        shutil.copyfileobj(response.raw, out_file)\n",
    "    del response\n",
    "    return(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def send_tweet(): \n",
    "    caption = get_meme_caption() \n",
    "    api.update_with_media(filename='img.png', status=caption)\n",
    "    print(\"Tweet sent!\") # If an error occurs, print it out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#schedule it\n",
    "schedule.every().day.at(\"10:00\").do(send_tweet)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Script for Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Convert to Script:\n",
    "#jupyter nbconvert --to script SimpsonsBot.ipynb\n",
    "\n",
    "#2.Create .sh rile:\n",
    "####\n",
    "#'#!/bin/bash;\n",
    "#'python /Users/whs/Desktop/StartUp/SimpsonsBot.py;\n",
    "####\n",
    "\n",
    "#3. Now make the .sh excecutable: \n",
    "#chmod a+x (yourscriptname.sh)\n",
    "\n",
    "#4. Trigger .sh file to run on startup :\n",
    "# MacOS: System Preferences >> Users and Groups >> Login Items\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
